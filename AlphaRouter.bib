@misc{applegateConcordeTspSolver2019,
  title = {Concorde Tsp Solver},
  author = {Applegate, David and Bixby, Robert and Chv{\'a}tal, Va{\v s}ek and Cook, William},
  year = {03.12.19}
}

@misc{belloNeuralCombinatorialOptimization2017,
  title = {Neural {{Combinatorial Optimization}} with {{Reinforcement Learning}}},
  author = {Bello, Irwan and Pham, Hieu and Le, Quoc V. and Norouzi, Mohammad and Bengio, Samy},
  year = {2017},
  month = jan,
  number = {arXiv:1611.09940},
  eprint = {1611.09940},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-04-07},
  abstract = {This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent neural network that, given a set of city coordinates, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent neural network using a policy gradient method. We compare learning the network parameters on a set of training graphs against learning them on individual test graphs. Despite the computational expense, without much engineering and heuristic designing, Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied to the KnapSack, another NP-hard problem, the same method obtains optimal solutions for instances with up to 200 items.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\admin\\Zotero\\storage\\H6YK8CIF\\Bello 등 - 2017 - Neural Combinatorial Optimization with Reinforceme.pdf}
}

@misc{bubeckRegretAnalysisStochastic2012,
  title = {Regret {{Analysis}} of {{Stochastic}} and {{Nonstochastic Multi-armed Bandit Problems}}},
  author = {Bubeck, S{\'e}bastien and {Cesa-Bianchi}, Nicol{\`o}},
  year = {2012},
  month = nov,
  number = {arXiv:1204.5721},
  eprint = {1204.5721},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-04-14},
  abstract = {Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration-exploitation trade-off. This is the balance between staying with the option that gave highest payoffs in the past and exploring new options that might give higher payoffs in the future. Although the study of bandit problems dates back to the Thirties, exploration-exploitation trade-offs arise in several modern applications, such as ad placement, website optimization, and packet routing. Mathematically, a multi-armed bandit is defined by the payoff process associated with each option. In this survey, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoffs and adversarial payoffs. Besides the basic setting of finitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\admin\\Zotero\\storage\\UFWTXTWH\\Bubeck_Cesa-Bianchi_2012_Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems.pdf;C\:\\Users\\admin\\Zotero\\storage\\G5UVJMKM\\1204.html}
}

@misc{daiLearningCombinatorialOptimization2018,
  title = {Learning {{Combinatorial Optimization Algorithms}} over {{Graphs}}},
  author = {Dai, Hanjun and Khalil, Elias B. and Zhang, Yuyu and Dilkina, Bistra and Song, Le},
  year = {2018},
  month = feb,
  number = {arXiv:1704.01665},
  eprint = {1704.01665},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-04-07},
  abstract = {The design of good heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires significant specialized knowledge and trial-and-error. Can we automate this challenging, tedious process, and learn the algorithms instead? In many real-world applications, it is typically the case that the same optimization problem is solved again and again on a regular basis, maintaining the same problem structure but differing in the data. This provides an opportunity for learning heuristic algorithms that exploit the structure of such recurring problems. In this paper, we propose a unique combination of reinforcement learning and graph embedding to address this challenge. The learned greedy policy behaves like a meta-algorithm that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show that our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\admin\\Zotero\\storage\\9STZ947N\\Dai 등 - 2018 - Learning Combinatorial Optimization Algorithms ove.pdf}
}

@book{helsgaunExtensionLinKernighanHelsgaunTSP,
title = "An Extension of the Lin-Kernighan-Helsgaun TSP Solver for Constrained Traveling Salesman and Vehicle Routing Problems: Technical report",
abstract = "This report describes the implementation of an extension of the Lin-Kernighan-Helsgaun TSP solver for solving constrained traveling salesman and vehicle routing problems. The extension, which is called LKH-3, is able to solve a variety of well-known problems, including the sequential ordering problem (SOP), the traveling repairman problem (TRP), variants of the multiple travel-ing salesman problem (mTSP), as well as vehicle routing problems (VRPs) with capacity, time windows, pickup-and-delivery and distance constraints. The implementation of LKH-3 builds on the idea of transforming the problems into standard symmetric traveling salesman problems and handling constraints by means of penalty functions. Extensive testing on benchmark instances from the literature has shown that LKH-3 is effective. Best known solutions are often obtained, and in some cases, new best solutions are found. The program is free of charge for academic and non-commercial use and can be downloaded in source code. A comprehensive library of bench-mark instances and the best obtained results for these instances can also be downloaded.",
author = "Keld Helsgaun",
year = "2017",
month = dec,
day = "29",
language = "English",
publisher = "Roskilde Universitet",
}

@misc{hottungEfficientActiveSearch2022,
  title = {Efficient {{Active Search}} for {{Combinatorial Optimization Problems}}},
  author = {Hottung, Andr{\'e} and Kwon, Yeong-Dae and Tierney, Kevin},
  year = {2022},
  month = mar,
  number = {arXiv:2106.05126},
  eprint = {2106.05126},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  urldate = {2023-04-14},
  abstract = {Recently numerous machine learning based methods for combinatorial optimization problems have been proposed that learn to construct solutions in a sequential decision process via reinforcement learning. While these methods can be easily combined with search strategies like sampling and beam search, it is not straightforward to integrate them into a high-level search procedure offering strong search guidance. Bello et al. (2016) propose active search, which adjusts the weights of a (trained) model with respect to a single instance at test time using reinforcement learning. While active search is simple to implement, it is not competitive with state-of-the-art methods because adjusting all model weights for each test instance is very time and memory intensive. Instead of updating all model weights, we propose and evaluate three efficient active search strategies that only update a subset of parameters during the search. The proposed methods offer a simple way to significantly improve the search performance of a given model and outperform state-of-the-art machine learning based methods on combinatorial problems, even surpassing the well-known heuristic solver LKH3 on the capacitated vehicle routing problem. Finally, we show that (efficient) active search enables learned models to effectively solve instances that are much larger than those seen during training.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {C\:\\Users\\admin\\Zotero\\storage\\QYWL8Z85\\Hottung et al_2022_Efficient Active Search for Combinatorial Optimization Problems.pdf;C\:\\Users\\admin\\Zotero\\storage\\TZD4646K\\2106.html}
}

@article{konstantakopoulosVehicleRoutingProblem2022,
  title = {Vehicle Routing Problem and Related Algorithms for Logistics Distribution: A Literature Review and Classification},
  shorttitle = {Vehicle Routing Problem and Related Algorithms for Logistics Distribution},
  author = {Konstantakopoulos, Grigorios D. and Gayialis, Sotiris P. and Kechagias, Evripidis P.},
  year = {2022},
  month = jul,
  journal = {Operational Research},
  volume = {22},
  number = {3},
  pages = {2033--2062},
  issn = {1866-1505},
  doi = {10.1007/s12351-020-00600-7},
  urldate = {2023-04-10},
  abstract = {The scheduling of deliveries and the routing of vehicles are of great importance for supply chain operations, as both determine to a great extent the distribution costs, as well as customer satisfaction. The fact that the distribution of goods is being affected by multiple factors, stemming from the demands of transportation companies, customers, and the external environment, has made the vehicle routing problem (VRP) among the most studied topics in operational research. These factors are transformed either to constraints or variables of the problem and finally lead to the creation of different variants of the VRP, formulated and studied by researchers. Moreover, the management of logistics and supply chain operations is being enhanced by the use of algorithms, integrated into information systems, enabling the optimization of real-life distribution cases. This paper presents a methodology for classifying the multiple VRP variants related to freight transportation, that most logistics and distribution companies face in their daily operations, as well as the algorithms solving the various problems. The application of the research methodology concluded to 334 papers, which were further sorted to 263 papers on the subject of freight transportation, aiming to identify the trends of the VRP variants and the applied algorithms, during the last decade. The correlation between the VRP variants and the applied algorithms is also identified. Finally, the paper presents the quantitative and qualitative results of the literature review and discusses the scientific publications with a significant impact on the research community.},
  langid = {english},
  keywords = {Algorithms,Freight transportation,Literature review,Logistics distribution,Vehicle routing problem,VRP},
  file = {C\:\\Users\\admin\\Zotero\\storage\\Q74ZR6VD\\Konstantakopoulos et al_2022_Vehicle routing problem and related algorithms for logistics distribution.pdf}
}

@inproceedings{kool2018attention,
  title={Attention, Learn to Solve Routing Problems!},
  author={Kool, Wouter and van Hoof, Herke and Welling, Max},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@misc{kumarSurveyVehicleRouting2012,
  title = {A {{Survey}} on the {{Vehicle Routing Problem}} and {{Its Variants}}},
  author = {Kumar, Suresh Nanda and Panneerselvam, Ramasamy},
  year = {2012},
  month = may,
  volume = {2012},
  publisher = {{Scientific Research Publishing}},
  doi = {10.4236/iim.2012.43010},
  urldate = {2023-04-13},
  abstract = {In this paper, we have conducted a literature review on the recent developments and publications involving the vehicle routing problem and its variants, namely vehicle routing problem with time windows (VRPTW) and the capacitated vehicle routing problem (CVRP) and also their variants. The VRP is classified as an NP-hard problem. Hence, the use of exact optimization methods may be difficult to solve these problems in acceptable CPU times, when the problem involves real-world data sets that are very large. The vehicle routing problem comes under combinatorial problem. Hence, to get solutions in determining routes which are realistic and very close to the optimal solution, we use heuristics and meta-heuristics. In this paper we discuss the various exact methods and the heuristics and meta-heuristics used to solve the VRP and its variants.},
  langid = {english},
  file = {C\:\\Users\\admin\\Zotero\\storage\\4D4FKXU6\\Kumar_Panneerselvam_2012_A Survey on the Vehicle Routing Problem and Its Variants.pdf}
}

@misc{kwonPOMOPolicyOptimization2021,
 author = {Kwon, Yeong-Dae and Choo, Jinho and Kim, Byoungjip and Yoon, Iljoo and Gwon, Youngjune and Min, Seungjai},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21188--21198},
 publisher = {Curran Associates, Inc.},
 title = {POMO: Policy Optimization with Multiple Optima for Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f231f2107df69eab0a3862d50018a9b2-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{linEffectiveHeuristicAlgorithm1973,
  title = {An {{Effective Heuristic Algorithm}} for the {{Traveling-Salesman Problem}}},
  author = {Lin, S. and Kernighan, B. W.},
  year = {1973},
  journal = {Operations Research},
  volume = {21},
  number = {2},
  eprint = {169020},
  eprinttype = {jstor},
  pages = {498--516},
  doi = {10.1287/opre.21.2.498},
  langid = {english},
  file = {C\:\\Users\\admin\\Zotero\\storage\\SFVTVMP6\\Lin 그리고 Kernighan - 1973 - An Effective Heuristic Algorithm for the Traveling.pdf}
}

@misc{luongEffectiveApproachesAttentionbased2015,
  title = {Effective {{Approaches}} to {{Attention-based Neural Machine Translation}}},
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  year = {2015},
  month = sep,
  number = {arXiv:1508.04025},
  eprint = {1508.04025},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-13},
  abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\admin\\Zotero\\storage\\JTKDC487\\Luong 등 - 2015 - Effective Approaches to Attention-based Neural Mac.pdf}
}

@article{schrittwieserMasteringAtariGo2020,
  title = {Mastering {{Atari}}, {{Go}}, {{Chess}} and {{Shogi}} by {{Planning}} with a {{Learned Model}}},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  year = {2020},
  month = dec,
  journal = {Nature},
  volume = {588},
  number = {7839},
  eprint = {1911.08265},
  primaryclass = {cs, stat},
  pages = {604--609},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-03051-4},
  urldate = {2023-04-07},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\admin\\Zotero\\storage\\PYPCWJML\\Schrittwieser 등 - 2020 - Mastering Atari, Go, Chess and Shogi by Planning w.pdf}
}

@article{silverMasteringGameGo2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  urldate = {2023-04-07},
  langid = {english},
  file = {C\:\\Users\\admin\\Zotero\\storage\\ACK8FAAD\\Silver 등 - 2016 - Mastering the game of Go with deep neural networks.pdf}
}

@article{silverMasteringGameGo2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  journal = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature24270},
  urldate = {2023-04-07},
  langid = {english},
  file = {C\:\\Users\\admin\\Zotero\\storage\\8JD7RQSP\\Silver 등 - 2017 - Mastering the game of Go without human knowledge.pdf}
}

@misc{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year = {2014},
  month = dec,
  number = {arXiv:1409.3215},
  eprint = {1409.3215},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-13},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\admin\\Zotero\\storage\\B4JECRTP\\Sutskever et al_2014_Sequence to Sequence Learning with Neural Networks.pdf;C\:\\Users\\admin\\Zotero\\storage\\ZWRTSEYP\\1409.html}
}

@article{suttonPolicyGradientMethods,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-14},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\admin\\Zotero\\storage\\M3YTMS9T\\1706.html}
}

@misc{vinyalsPointerNetworks2017,
  title = {Pointer {{Networks}}},
  author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  year = {2017},
  month = jan,
  number = {arXiv:1506.03134},
  eprint = {1506.03134},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-04-07},
  abstract = {We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems \textendash{} finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem \textendash{} using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\admin\\Zotero\\storage\\NDVY36CS\\Vinyals 등 - 2017 - Pointer Networks.pdf}
}

@article{williamsREINFORCE,
author = {Williams, Ronald J.},
title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
year = {1992},
issue_date = {May 1992},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {3–4},
issn = {0885-6125},
url = {https://doi.org/10.1007/BF00992696},
doi = {10.1007/BF00992696},
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
journal = {Mach. Learn.},
month = {may},
pages = {229–256},
numpages = {28},
keywords = {gradient descent, mathematical analysis, connectionist networks, Reinforcement learning}
}

@article{kwonMatNet,
  title={Matrix encoding networks for neural combinatorial optimization},
  author={Kwon, Yeong-Dae and Choo, Jinho and Yoon, Iljoo and Park, Minah and Park, Duwon and Gwon, Youngjune},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={5138--5149},
  year={2021}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@misc{CVRPLP,
    title={Two models of the capacitated vehicle routing problem},
      author={Borcinova, Zuzana},
      journal={Croatian Operational Research Review},
      pages={463--469},
      year={2017}
}

@article{agarap2018ReLU,
  title={Deep learning using rectified linear units (relu)},
  author={Agarap, Abien Fred},
  journal={arXiv preprint arXiv:1803.08375},
  year={2018}
}

@article{shazeer2020swiglu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{paszke2017automatic,
  title={Automatic differentiation in pytorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017},
}

@misc {pytorchLightning,
    title = {PyTorch Lightning, opensource},
    author = {The PyTorch Lightning team},
    url = {https://www.pytorchlightning.ai},
    version = {1.4},
    note = {https://github.com/Lightning-AI/lightning},
    year = {2023}
}

@article{schrittwieser2020mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{parallel_MCTS,
  title={Parallel monte-carlo tree search},
  author={Chaslot, Guillaume MJ -B and Winands, Mark HM and van Den Herik, H Jaap},
  booktitle={Computers and Games: 6th International Conference, CG 2008, Beijing, China, September 29-October 1, 2008. Proceedings 6},
  pages={60--71},
  year={2008},
  organization={Springer}
}

@article{dynamicVRP,
  title={Dynamic VRPs: A study of scenarios},
  author={Kilby, Philip and Prosser, Patrick and Shaw, Paul},
  journal={University of Strathclyde Technical Report},
  volume={1},
  number={11},
  year={1998}
}

@article{tensor_mult_RL,
author = {Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Ruiz, Francisco and Schrittwieser, Julian and Swirszcz, Grzegorz and Silver, David and Hassabis, Demis and Kohli, Pushmeet},
year = {2022},
month = {10},
pages = {47-53},
title = {Discovering faster matrix multiplication algorithms with reinforcement learning},
volume = {610},
journal = {Nature},
doi = {10.1038/s41586-022-05172-4}
}

@article{HGS,
author = {Vidal, Thibaut and Crainic, Teodor Gabriel and Gendreau, Michel and Lahrichi, Nadia and Rei, Walter},
year = {2012},
month = {06},
pages = {611-624},
title = {A Hybrid Genetic Algorithm for Multidepot and Periodic Vehicle Routing Problems},
volume = {60},
journal = {Operations Research},
doi = {10.1287/opre.1120.1048}
}

@article{hybridGeneticSearch_Vidal,
title = {Hybrid genetic search for the CVRP: Open-source implementation and SWAP* neighborhood},
journal = {Computers \& Operations Research},
volume = {140},
pages = {105643},
year = {2022},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2021.105643},
url = {https://www.sciencedirect.com/science/article/pii/S030505482100349X},
author = {Thibaut Vidal},
keywords = {Vehicle routing problem, Neighborhood search, Hybrid genetic search, Open source},
abstract = {The vehicle routing problem is one of the most studied combinatorial optimization topics, due to its practical importance and methodological interest. Yet, despite extensive methodological progress, many recent studies are hampered by the limited access to simple and efficient open-source solution methods. Given the sophistication of current algorithms, reimplementation is becoming a difficult and time-consuming exercise that requires extensive care for details to be truly successful. Against this background, we use the opportunity of this short paper to introduce a simple – open-source – implementation of the hybrid genetic search (HGS) specialized to the capacitated vehicle routing problem (CVRP). This state-of-the-art algorithm uses the same general methodology as Vidal et al. (2012) but also includes additional methodological improvements and lessons learned over the past decade of research. In particular, it includes an additional neighborhood called SWAP* which consists in exchanging two customers between different routes without an insertion in place. As highlighted in our study, an efficient exploration of SWAP* moves significantly contributes to the performance of local searches. Moreover, as observed in experimental comparisons with other recent approaches on the classical instances of Uchoa et al. (2017), HGS still stands as a leading metaheuristic regarding solution quality, convergence speed, and conceptual simplicity.}
}

@article{QubitRouting_with_MCTS, title={Qubit Routing Using Graph Neural Network Aided Monte Carlo Tree Search}, volume={36}, url={https://ojs.aaai.org/index.php/AAAI/article/view/21231}, DOI={10.1609/aaai.v36i9.21231}, abstractNote={Near-term quantum hardware can support two-qubit operations only on the qubits that can interact with each other. Therefore, to execute an arbitrary quantum circuit on the hardware, compilers have to first perform the task of qubit routing, i.e., to transform the quantum circuit either by inserting additional SWAP gates or by reversing existing CNOT gates to satisfy the connectivity constraints of the target topology. The depth of the transformed quantum circuits is minimized by utilizing the Monte Carlo tree search (MCTS) to perform qubit routing by making it both construct each action and search over the space of all actions. It is aided in performing these tasks by a Graph neural network that evaluates the value function and action probabilities for each state. Along with this, we propose a new method of adding mutex-lock like variables in our state representation which helps factor in the parallelization of the scheduled operations, thereby pruning the depth of the output circuit. Overall, our procedure (referred to as QRoute) performs qubit routing in a hardware agnostic manner, and it outperforms other available qubit routing implementations on various circuit benchmarks.}, number={9}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Sinha, Animesh and Azad, Utkarsh and Singh, Harjinder}, year={2022}, month={Jun.}, pages={9935-9943} }


@article{AlphaRouter_AAAI, title={AlphaRoute: Large-Scale Coordinated Route Planning via Monte Carlo Tree Search}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/26422}, DOI={10.1609/aaai.v37i10.26422}, abstractNote={This paper proposes AlphaRoute, an AlphaGo inspired algorithm for coordinating large-scale routes, built upon graph attention reinforcement learning and Monte Carlo Tree Search (MCTS). We first partition the road network into regions and model large-scale coordinated route planning as a Markov game, where each partitioned region is treated as a player instead of each driver. Then, AlphaRoute applies a bilevel optimization framework, consisting of several region planners and a global planner, where the region planner coordinates the route choices for vehicles located in the region and generates several strategies, and the global planner evaluates the combination of strategies. AlphaRoute is built on graph attention network for evaluating each state and MCTS algorithm for dynamically visiting and simulating the future state for narrowing down the search space. AlphaRoute is capable of 1) bridging user fairness and system efficiency, 2) achieving higher search efficiency by alleviating the curse of dimensionality problems, and 3) making an effective and informed route planning by simulating over the future to capture traffic dynamics. Comprehensive experiments are conducted on two real-world road networks as compared with several baselines to evaluate the performance, and results show that AlphaRoute achieves the lowest travel time, and is efficient and effective for coordinating large-scale routes and alleviating the traffic congestion problem. The code will be publicly available.}, number={10}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Luo, Guiyang and Wang, Yantao and Zhang, Hui and Yuan, Quan and Li, Jinglin}, year={2023}, month={Jun.}, pages={12058-12067} }

@misc{ortools,
  title = {OR-Tools Routing Library},
  version = { v9.8 },
  author = {Vincent Furnon and Laurent Perron},
  organization = {Google},
  url = {https://developers.google.com/optimization/routing/},
  date = { 2023-11-15 },
  year= {2023}
}

@article{nearset_insertion,
author = {Rosenkrantz, Daniel and Stearns, Richard and II, Philip},
year = {1977},
month = {09},
pages = {563-581},
title = {An Analysis of Several Heuristics for the Traveling Salesman Problem},
volume = {6},
isbn = {978-1-4020-9687-7},
journal = {SIAM J. Comput.},
doi = {10.1137/0206041}
}

@inproceedings{masked_attention,
  title={Masked-attention mask transformer for universal image segmentation},
  author={Cheng, Bowen and Misra, Ishan and Schwing, Alexander G and Kirillov, Alexander and Girdhar, Rohit},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={1290--1299},
  year={2022}
}

@article{masked_attention2,
title = {Full transformer network with masking future for word-level sign language recognition},
journal = {Neurocomputing},
volume = {500},
pages = {115-123},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.05.051},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222006178},
author = {Yao Du and Pan Xie and Mingye Wang and Xiaohui Hu and Zheng Zhao and Jiaqi Liu},
keywords = {Word-level sign language recognition, Transformer, Mask Future},
abstract = {Word-level sign language recognition (SLR) is a significant task which transcribes a sign language video into a word. Currently, deep-learning-based frameworks mostly combine spatial feature extractors based on convolution neural networks (CNNs) and sequence learners. These methods either lack the sufficient capacity to establish the high-level vision semantic knowledge and incorporate the details in images or perform weak intelligence on video frame sequence comprehension. Focusing on gestures and facial expressions is essential to interpreting sign language; however, it is challenging to crop these elements from pictures and distill them end-to-end. In this paper, a full self-attention framework for word-level SLR is proposed to tackle the above issue, which integrates a Vision Transformer as spatial encoder and an improved temporal Transformer. In addition, we introduce the masking future operation to improve the Transformer for the temporal module. The vision Transformer first refines the latent high-level semantic feature sequences from sign language videos and feeds them into the temporal module. Then the masking future Transformer enhances this sequence by making subsequent time invisible at each moment of frames and generates the final recognition. This approach integrates global and local spatial information; furthermore, it can also distinguish the latent semantic features contained in sign language action sequences. To validate the proposed approach, we perform extensive experiments on two datasets. The results and ablation studies demonstrate the effectiveness of this method, and it achieves new state-of-the-art performance on the WLASL dataset by using RGB images alone.}
}